{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YLL_VGG_16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanli499/APS360-Project/blob/Lucy_1/YLL_VGG_16_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48t6NUyZ_hqq",
        "colab_type": "text"
      },
      "source": [
        "YLL Test Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ms--CVQ918x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ALL import statements\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.models\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTfCIazf_dAb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e019196b-add1-482b-cb01-b7376efbbf2d"
      },
      "source": [
        "# Mount our Google Drive\n",
        "# re-run whenever needed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDjCaW97_rWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# re-run when needed\n",
        "# classes are folders in each directory with these names\n",
        "classes = ['afraid','angry','disgusted','happy','neutral','sad','surprised']\n",
        "\n",
        "# emotion label for KDEF photos\n",
        "emotion_code = {\"AF\":\"afraid\", \"AN\":\"angry\", \"DI\":\"disgusted\", \"HA\":\"happy\", \n",
        "                \"NE\":\"neutral\", \"SA\":\"sad\", \"SU\":\"surprised\"}\n",
        "\n",
        "data_dir='/content/drive/My Drive/Colab Notebooks/Faces'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLljEYqZAWVz",
        "colab_type": "code",
        "outputId": "3600dc01-f253-4d61-be78-e9953a4a55f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# RUN ONLY ONCE!!!\n",
        "\n",
        "# logic for sorting thru KDEF dataset for the images we want\n",
        "\"\"\"\n",
        "- eg file name: AF01ANFL.JPG\n",
        "- Check:\n",
        "    - length of name = 7, for straight profile only, ends with \"S.jpg\"\n",
        "    - str[4:5] = {\"AF\":\"afraid\", \"AN\":\"angry\", \"DI\":\"disgusted\", \"HA\":\"happy\",\n",
        "    \"NE\":\"neutral\", \"SA\":sad\", \"SU\":\"surprised\"}\n",
        "\"\"\"\n",
        "\n",
        "# delete existing folder\n",
        "if os.path.exists(data_dir+'/'):\n",
        "    shutil.rmtree(data_dir+'/')\n",
        "\n",
        "# make new directories for each emotion class + train, val, test\n",
        "try:\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'/'+classes[i])\n",
        "\n",
        "    os.mkdir(data_dir+'/train')\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'/train/'+classes[i])\n",
        "    \n",
        "    os.mkdir(data_dir+'/val')\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'/val/'+classes[i])\n",
        "\n",
        "    os.mkdir(data_dir+'/test')\n",
        "    for i in range(len(classes)):\n",
        "        os.mkdir(data_dir+'/test/'+classes[i])\n",
        "\n",
        "except OSError:\n",
        "    print (\"Creation of the directories failed!\")\n",
        "else:\n",
        "    print (\"Successfully created the directories!\")\n",
        "\n",
        "# rootdir = path to KDEF main folder\n",
        "rootdir = '/content/drive/My Drive/Colab Notebooks/PROJECT/KDEF/'\n",
        "\n",
        "# go thru KDEF data + sort out desired photos\n",
        "for subdir, dirs, files in os.walk(rootdir):\n",
        "    for file in files:\n",
        "        filename = subdir + os.sep + file\n",
        "        if (file.endswith(\"S.jpg\") or file.endswith(\"S.JPG\")): \n",
        "            # for each straight profile photo:\n",
        "            # convert RGB --> Grayscale\n",
        "            # resize to 256 x 256 pixels, b/c will center crop to 224 x 224 later\n",
        "            # then save in the corresponding emotion class folder\n",
        "            img = Image.open(filename).convert('L')\n",
        "            new_img = img.resize((256, 256))\n",
        "\n",
        "            if (file[4:6] == \"AF\"):\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"AF\"]+'/'+file)\n",
        "            elif (file[4:6] == \"AN\"):\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"AN\"]+'/'+file)\n",
        "            elif (file[4:6] == \"DI\"): \n",
        "                new_img.save(data_dir+'/'+emotion_code[\"DI\"]+'/'+file)\n",
        "            elif (file[4:6] == \"HA\"): \n",
        "                new_img.save(data_dir+'/'+emotion_code[\"HA\"]+'/'+file)\n",
        "            elif (file[4:6] == \"NE\"):\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"NE\"]+'/'+file)\n",
        "            elif (file[4:6] == \"SA\"): \n",
        "                new_img.save(data_dir+'/'+emotion_code[\"SA\"]+'/'+file)\n",
        "            elif (file[4:6] == \"SU\"):\n",
        "                new_img.save(data_dir+'/'+emotion_code[\"SU\"]+'/'+file)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully created the directories!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ONtGEPi4oIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data into train, val, test datasets (60:20:20)\n",
        "# each class = 140 images --> 84 train, 28 val, 28 test\n",
        "\n",
        "# divide data into train, val, + test\n",
        "# for each emotion class, get filenames, shuffle, \n",
        "# divide, move to corresponding folders in train, val, test\n",
        "for c in classes:\n",
        "    filepath = data_dir+'/'+c\n",
        "    names = []\n",
        "\n",
        "    for file in os.listdir(filepath):\n",
        "        names.append(file)\n",
        "    random.shuffle(names)\n",
        "\n",
        "    # TODO: Can probably simplify this\n",
        "    count = 0\n",
        "    for name in names:\n",
        "        if(count == (84-42)):\n",
        "            break\n",
        "        else:\n",
        "            # shutil.move(dir1, dir2) = moves file form directory 1 to directory 2\n",
        "            shutil.move(filepath+'/'+name, data_dir+'/train/'+c+'/'+name)\n",
        "            names.remove(name)\n",
        "            count += 1\n",
        "\n",
        "    count = 0\n",
        "    for name in names:\n",
        "        if(count == (28-14)):\n",
        "            break\n",
        "        else:\n",
        "            shutil.move(filepath+'/'+name, data_dir+'/val/'+c+'/'+name)\n",
        "            names.remove(name)\n",
        "            count += 1\n",
        "\n",
        "    count = 0\n",
        "    for name in names:\n",
        "        if(count == (28-14)):\n",
        "            break\n",
        "        else:\n",
        "            shutil.move(filepath+'/'+name, data_dir+'/test/'+c+'/'+name)\n",
        "            names.remove(name)\n",
        "            count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWTrH-BE1Jis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# crop all images to 224 x 224 for all datasets\n",
        "# generate image folders + data loaders for train, val, test\n",
        "data_transform = transforms.Compose([transforms.CenterCrop(224), transforms.ToTensor()])\n",
        "batch_size = 30\n",
        "\n",
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(\n",
        "        os.path.join(data_dir, 'train/'), \n",
        "        transform=data_transform\n",
        "    ),\n",
        "    'val': datasets.ImageFolder(\n",
        "        os.path.join(data_dir, 'val/'), \n",
        "        transform=data_transform\n",
        "    ),\n",
        "    'test': datasets.ImageFolder(\n",
        "        os.path.join(data_dir, 'test/'), \n",
        "        transform=data_transform\n",
        "    )\n",
        "}\n",
        "\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(\n",
        "        image_datasets['train'], batch_size=batch_size\n",
        "    ),\n",
        "    'val': torch.utils.data.DataLoader(\n",
        "        image_datasets['val'], batch_size=batch_size\n",
        "    ),\n",
        "    'test': torch.utils.data.DataLoader(\n",
        "        image_datasets['test'], batch_size=batch_size\n",
        "    )\n",
        "}\n",
        "\n",
        "# get size of each dataset\n",
        "dataset_sizes = {\n",
        "    'train': len(image_datasets['train']),\n",
        "    'val': len(image_datasets['val']),\n",
        "    'test': len(image_datasets['test']) \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHz0DgU4Czqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From: https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch\n",
        "def train_model(vgg, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(vgg.state_dict())\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    avg_loss = 0\n",
        "    avg_acc = 0\n",
        "    avg_loss_val = 0\n",
        "    avg_acc_val = 0\n",
        "    \n",
        "    train_batches = len(dataloaders[\"train\"])\n",
        "    val_batches = len(dataloaders[\"val\"])\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "        \n",
        "        loss_train = 0\n",
        "        loss_val = 0\n",
        "        acc_train = 0\n",
        "        acc_val = 0\n",
        "        \n",
        "        vgg.train(True)\n",
        "        \n",
        "        for i, data in enumerate(dataloaders[\"train\"]):\n",
        "            if i % 100 == 0:\n",
        "                print(\"\\rTraining batch {}/{}\".format(i, train_batches / 2), end='', flush=True)\n",
        "                \n",
        "            # Use half training dataset\n",
        "            # if i >= train_batches / 2:\n",
        "            #     break\n",
        "                \n",
        "            inputs, labels = data\n",
        "            \n",
        "            # if use_gpu:\n",
        "            #     inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "            # else:\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = vgg(inputs)\n",
        "            \n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            loss_train += loss\n",
        "            acc_train += torch.sum(preds == labels.data)\n",
        "            \n",
        "            del inputs, labels, outputs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        print()\n",
        "        # * 2 as we only used half of the dataset\n",
        "        avg_loss = loss_train / dataset_sizes[\"train\"]\n",
        "        avg_acc = acc_train / dataset_sizes[\"train\"]\n",
        "        \n",
        "        vgg.train(False)\n",
        "        vgg.eval()\n",
        "            \n",
        "        for i, data in enumerate(dataloaders[\"val\"]):\n",
        "            if i % 100 == 0:\n",
        "                print(\"\\rValidation batch {}/{}\".format(i, val_batches), end='', flush=True)\n",
        "                \n",
        "            inputs, labels = data\n",
        "            \n",
        "            # if use_gpu:\n",
        "            #     inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n",
        "            # else:\n",
        "            inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = vgg(inputs)\n",
        "            \n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            loss_val += loss\n",
        "            acc_val += torch.sum(preds == labels.data)\n",
        "            \n",
        "            del inputs, labels, outputs, preds\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        avg_loss_val = loss_val / dataset_sizes[\"val\"]\n",
        "        avg_acc_val = acc_val / dataset_sizes[\"val\"]\n",
        "        \n",
        "        print()\n",
        "        print(\"Epoch {} result: \".format(epoch))\n",
        "        print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n",
        "        print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n",
        "        print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n",
        "        print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n",
        "        print('-' * 10)\n",
        "        print()\n",
        "        \n",
        "        if avg_acc_val > best_acc:\n",
        "            best_acc = avg_acc_val\n",
        "            best_model_wts = copy.deepcopy(vgg.state_dict())\n",
        "        \n",
        "    elapsed_time = time.time() - since\n",
        "    print()\n",
        "    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time // 60, elapsed_time % 60))\n",
        "    print(\"Best acc: {:.4f}\".format(best_acc))\n",
        "    \n",
        "    vgg.load_state_dict(best_model_wts)\n",
        "    return vgg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dwEF8nz--KU",
        "colab_type": "code",
        "outputId": "153a0c57-3624-4d77-d2c2-25341e2bc4e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        }
      },
      "source": [
        "# get pretrained vgg16 model\n",
        "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "# replace last layer in classifier to change output as needed\n",
        "num_features = vgg16.classifier[6].in_features\n",
        "features = list(vgg16.classifier.children())[:-1]\n",
        "features.extend([nn.Linear(num_features, len(classes))])\n",
        "vgg16.classifier = nn.Sequential(*features) # Replace the model classifier\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(vgg16.parameters(), lr=0.003, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "vgg16 = train_model(vgg16, criterion, optimizer_ft, exp_lr_scheduler)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/2\n",
            "----------\n",
            "Training batch 0/5.0\n",
            "Validation batch 0/4"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 0 result: \n",
            "Avg loss (train): 0.0778\n",
            "Avg acc (train): 0.0000\n",
            "Avg loss (val): 0.0788\n",
            "Avg acc (val): 0.0000\n",
            "----------\n",
            "\n",
            "Epoch 1/2\n",
            "----------\n",
            "Training batch 0/5.0\n",
            "Validation batch 0/4\n",
            "Epoch 1 result: \n",
            "Avg loss (train): 0.0725\n",
            "Avg acc (train): 0.0000\n",
            "Avg loss (val): 0.0796\n",
            "Avg acc (val): 0.0000\n",
            "----------\n",
            "\n",
            "\n",
            "Training completed in 8m 52s\n",
            "Best acc: 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bCJlYni8kel",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7c42041f-ff98-4090-ca3a-fcee58c02905"
      },
      "source": [
        "\"\"\"\n",
        "NOTE:\n",
        "The VGG-16 is able to classify 1000 different labels; we just need 4 instead.\n",
        "In order to do that we are going replace the last fully connected layer of the \n",
        "model with a new one with 4 output features instead of 1000.\n",
        "\n",
        "In PyTorch, we can access the VGG-16 classifier with model.classifier, \n",
        "which is an 6-layer array. We will replace the last entry.\n",
        "\"\"\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNOTE:\\nThe VGG-16 is able to classify 1000 different labels; we just need 4 instead.\\nIn order to do that we are going replace the last fully connected layer of the \\nmodel with a new one with 4 output features instead of 1000.\\n\\nIn PyTorch, we can access the VGG-16 classifier with model.classifier, \\nwhich is an 6-layer array. We will replace the last entry.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}